\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{array}
\usepackage{float}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{enumitem}

%\eaclfinalcopy
\usepackage[compact]{titlesec}         % you need this package
\titlespacing{\section}{0pt}{0pt}{0pt} % this reduces space between (sub)sections to 0pt, for example
\titlespacing{\subsection}{0pt}{0pt}{0pt} % this reduces space between (sub)sections to 0pt, for example
\titlespacing{\subsubsection}{0pt}{0pt}{0pt} % this reduces space between (sub)sections to 0pt, for example
\AtBeginDocument{%                     % this will reduce spaces between parts (above and below) of texts within a (sub)section to 0pt, for example - like between an 'eqnarray' and text
  \setlength\abovedisplayskip{0pt}
  \setlength\belowdisplayskip{0pt}}
  
\captionsetup[table]{name=Figure}
\captionsetup{belowskip=-20pt,aboveskip=4pt}
\captionsetup[figure]{font=small,labelfont=footnotesize}
\captionsetup[table]{font=small,labelfont=footnotesize}

\begin{document}
\nocite{MARTHA, Vickrey, Clark, Quants, Hannaneh}

\title{Using Sentence Simplification to Solve Arithmetic Word Problems}

\author{First Author \\
  Affiliation / Address line 1 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  {\tt email@domain} \\}

\date{}
\maketitle

\begin{abstract}
This paper presents a sentence simplification based approach for learning to solve arithmetic word problems. In an effort to reduce parsing and classification errors, we begin by using linguistic properties to process the text such that each sentence represents a single mathematical operation.  Based on the simplified sentences, a classifier is learned to predict operators for each simplified sentence that is used to build an equation to solve the problem. On the MAWPS~\cite{MAWPS} addition and subtraction problems, we demonstrate performance competitive with existing state of the art systems. 
\end{abstract}

\section{Introduction}
Interpreting a sentence representing a single mathematical operation is simpler and less error prone than interpreting a sentence having multiple mathematical operations. For example, consider Figure \ref{figure:1} where splitting the first sentence into two sentences leads to a more straightforward analysis.  To simplify the word problem, we execute a set of rules on each sentence to possibly produce multiple simplified sentences with the goal of a single mathematical operation associated with each simplified sentence.  We then learn a operator classifier based on these simplified sentences and generate a solution equation.

\begin{table}[h]
\fontsize{9}{9}
\begin{tabularx}{7.9cm}{|c|c|}
\hline 
\multicolumn{2}{|c|}{\bf{Example Word Problem}} \\ \hline
\multicolumn{2}{|p{7cm}|}{\small A spaceship traveled 0.5 light-year from Earth to Planet X and 0.1 light-year from Planet X to Planet Y. How many light-years did the spaceship travel in all ?} \\ \hline
\multicolumn{1}{|m{5cm}|}{\bf \centering Simplified Sentence} & \multicolumn{1}{m{2cm}|}{\bf \centering Predicted Operations} \\ \hline
\multicolumn{1}{|m{5cm}|}{\small A spaceship traveled 0.5 light-year from Earth to Planet X.} & \multicolumn{1}{m{2cm}|}{\small \centering + 0.5 light-year} \\ \hline
\multicolumn{1}{|m{5cm}|}{\small A spaceship traveled 0.1 light-year from Planet X to Planet Y.} & \multicolumn{1}{m{2cm}|}{\small \centering + 0.1 light-year} \\ \hline
\multicolumn{2}{|m{7cm}|}{\small \centering \textbf{Equation:}  + 0.5 light-year + 0.1 light-year} \\ \hline
\end{tabularx}
\caption{\small \label{figure:1} Equation Extraction from Simplified Sentences }
\end{table}

\section{Related Work}
There have been a number of attempts to solve arithmetic word problems through machine learning (ML). Template based methods (e.g., \cite{Kushman:14}) implicitly assumes that the solution will be generated from a set of predefined equation templates.  Non-template based methods (e.g., \cite{ARIS:14,RoyT:15,RoyR:15}) use different methods to extract similar information. Based on different representations of the extracted information, an equation is generated for the problem text.  %Some of these methods only solve addition and subtraction problems (e.g., ~\cite{ARIS:14} and ~\cite{RoyT:15}) while others (e.g., ~\cite{RoyR:15} and ~\cite{Kushman:14}) can also solve problems that require multiplication/division operations. 
Our approach is distinct in that it uses sentence simplification to predict operators to handle addition and subtraction problems.

\section{Our Method}
In this section we describe how our system maps an arithmetic word problem to an equation. It consists of three main steps:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item Extract simplified sentences from complex word problems using the simplification rules. 
\item Train a model to classify operators for each simplified sentence.
\item Solve the problem by updating problem state with learned operators and create equations.
\end{enumerate}

\subsection{Sentence Simplification and Problem Decomposition}
Sentences in an arithmetic word problem are sometimes complex. Hence, it is difficult to extract information from such sentences. Even more challenging is to predict the impact of the sentence on the result. We extract a total of 1218 addition subtraction problems from the MAWPS repository ~\cite{MAWPS} and execute sentence simplification on all of them. We also release a dataset of simplified sentences for these word problems.\footnote{URL not provided to maintain anonymity.} We create a mapping for each sentence in the problem text to its simplified sentences by extracting their relational dependencies from the Stanford dependency parser. Currently, our system simplifies sentences based on conjunctions and punctuation characters such as comma. There are certain rules when simplifying the sentence as described in Section \ref{sec:SimplificationRules}.

\textbf{Notation:} Given a problem text $\mathbf{S}$, let the sentences in the $\mathbf{S}$ be $\mathbf{<s_{1},..., s_{n}>}$. Each sentence $\mathbf{s_{i}}$ will be simplified to \begin{math}m\end{math} simplified sentences. Let the simplified sentences of $\mathbf{s_{i}}$ be $\mathit{<k_{1},..., k_{m}>}$.

\subsubsection{Rules for Simplifying Sentences}\label{sec:SimplificationRules}
When the conjunction \textit{"and"} or  the punctuation character \textit{","} is encountered, our system attempts to create two simplified sentences from the actual sentence. The first sentence is the part before these elements while the second sentence is the part after them. Notably, after the split there may be some words which would be required in the second sentence. Consider the sentence in Figure \ref{figure:2}:
\vspace{-0.7cm}
\begin{table}[H]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
$\mathbf{s}$: \small The school cafeteria ordered 42 red apples and 7 green apples for students lunches.\\
\hline
\end{tabular}
\caption{\small Example Sentence}
\label{figure:2}
\end{table}
\vspace{0.3cm}
In s, the split based on \textit{"and"} will result in two sentences as shown below:
\\
\\
$\mathit{k_{1}}$:\textit{The school cafeteria ordered 42 red apples}\\
$\mathit{k_{2}}$:\textit{7 green apples for students lunches}
\\
Here $\mathit{k_{1}}$ has a subject and a verb while $\mathit{k_{2}}$ does not have them, making it an improper sentence. Hence, there are some rules for adding words to simplified sentences:

\subsubsection{Rules for adding words to simplified sentences.}\label{sec:secondsentencesimplification}
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item \label{rule:1}
If $\mathit{k_{1}}$ starts with an existential and has a verb after it and if $\mathit{k_{2}}$ does not have either expletive or verb, distribute them to $\mathit{k_{2}}$. Consider the example in Figure \ref{figure:3}:
\vspace{0cm}
\begin{table}[h!]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
 $\mathbf{\small s}$\textbf{\small : There were 2 siamese cats and 4 house cats.}\\
\hline
$\mathit{\small k_{1}}$\small : There were 2 siamese cats.\\
\hline
$\mathit{\small k_{2}}$\small : There were 2 house cats.\\
\hline
\end{tabular}
\caption{Example sentence for Rule \ref{rule:1}}.
\label{figure:3}
\end{table}
\vspace{-1.20cm}
\item \label{rule:2}
\vspace{1.1cm}
If $\mathit{k_{1}}$ starts with a noun, and if $\mathit{k_{2}}$ starts with a verb, the noun from the former will be distributed to the latter. Refer to an example in Figure \ref{figure:4}.
\vspace{-0.2cm}
\begin{table}[h!]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
 $\mathbf{\small s}$\textbf{\small : Joan ate 2 oranges and threw 3 apples.}\\
\hline
$\mathit{\small k_{1}}$\small : Joan ate 2 oranges.\\
\hline
$\mathit{\small k_{2}}$\small : Joan threw 3 apples.\\
\hline
\end{tabular}
\caption{Example sentence for Rule \ref{rule:2}.}
\label{figure:4}
\end{table}
\vspace{0.5cm}
\item  \label{rule:3}
If $\mathit{k_{1}}$ starts with a noun and $\mathit{k_{2}}$ has a \textit{noun verb} pattern, do nothing.
\vspace{-0.2cm}
\begin{table}[h!]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
 $\mathbf{\small s}$\textbf{\small : Tom has 9 yellow balloons and Sara has 8 yellow balloons.}\\
\hline
$\mathit{\small k_{1}}$\small : Tom has 9 yellow balloons.\\
\hline
$\mathit{\small k_{2}}$\small : Sara has 8 yellow balloons.\\
\hline
\end{tabular}
\caption{Example sentence for Rule  \ref{rule:3}.}
\label{figure:5}
\end{table}

In the example presented in Figure \ref{figure:5}, No words from $\mathit{k_{1}}$ were added to $\mathit{k_{2}}$ since it had the \textit{noun verb (Sara has)} pattern.

\item \label{rule:4}
If $\mathit{k_{2}}$ contains a preposition at the end and $\mathit{k_{1}}$ does not, it will be distributed from $\mathit{k_{2}}$ to $\mathit{k_{1}}$. Consider the example presented in Figure \ref{figure:6}:
\vspace{-0.2cm}
\begin{table}[h!]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
 $\mathbf{\small s}$\textbf{\small : Joan found 6 seashells and Jessica found 8 seashells on the beach.}\\
\hline
$\mathit{\small k_{1}}$\small : Joan found 6 seashells on the beach.\\
\hline
$\mathit{\small k_{2}}$\small : Jessica found 8 seashells on the beach.\\
\hline
\end{tabular}
\caption{Example sentence for Rule  \ref{rule:4}.}
\label{figure:6}
\end{table}

After splitting the sentence based on \textit{and}, the preposition and the words after it \textit{on the beach} were added to the first sentence.

\item
Based on the output by the dependency parser and our rules, there might be some words which might not have been identified. But we still need those words in the simplified sentences. Therefore, the sentence simplification system identifies all such words. If these words appear before the conjunction, they are added to $\mathit{k_{1}}$ at the correct index and if they appear after the conjunction, they are added to $\mathit{k_{2}}$.

\end{enumerate}

\section{Operation Prediction Classifier}

After all the sentences are simplified, we randomly divide the dataset into training and testing in the ratio of 3:1. We train our model using Random Forest classifier that predicts one of the following classes for each simplified sentence in the word problem:

\begin{table}[h!]
\centering
\begin{tabular}{ | m{1.6cm} | m{4cm} |}
\hline
\textbf{\small Class Label} & \textbf{\small Description}\\ \hline
\small + & \small Addition Operation.\\ \hline
\small - & \small Subtraction Operation\\ \hline
\small ? & \small Fragment asking some question\\ \hline
\small = & \small Assignment Operation\\ \hline
\small i & \small Irrelevant information \\ \hline
\end{tabular}
\caption{Labels for Operator Prediction Classifier}
\label{figure:7}
\end{table}

\subsection{Features}
\subsubsection{Position based}
The index of simplified sentence in the question is important to determine the operation that sentence will perform. We take 2 such features into consideration as shown in Figure \ref{figure:8}
\begin{table}[h!]
\centering
\begin{tabular}{ | m{3cm} | m{4cm} |}
\hline
\textbf{\small Feature} & \textbf{\small Description}\\ \hline
\small IsItAFirstSentence & \small Most word problems in the training data had a positive operation in the first sentence.\\ \hline
\small IsItALastSentence & \small Almost always the last sentence in the word problem is a question sentence.\\ \hline
\end{tabular}
\caption{Position based Features}
\label{figure:8}
\end{table}
\vspace{0.25cm}
\subsubsection{Relation based}
Existence of some important dependency relations is used as a feature. Refer to Figure \ref{figure:9} for the list of relation based features:
\begin{table}[H]
\begin{tabular}{|m{2cm} | m{4cm}|}
\hline 
\textbf{\small Feature} & \textbf{\small Description}\\ \hline
\small nsubj & \multirow{2}{4cm}{\small The sentence is more likely to perform an operation in the presence of these two relations.} \\[10pt]
\cline{1-1} 
 \small dobj & \\[10pt]
\hline
\end{tabular}
\caption{Relation based Features}
\label{figure:9}
\end{table}
\vspace{0.15cm}
\subsubsection{Parts of Speech based}
Existence of some Parts of Speech of the words in the sentence is used as a feature. Refer to Figure \ref{figure:10} for the list of POS based features:

\begin{table}[H]
\centering
\begin{tabular}{ | m{3cm} | m{4cm} |}
\hline
\textbf{\small Feature} & \textbf{\small Description}\\ \hline
\small CD: Cardinal & \multirow{8}{4cm}{\centering The sentence is more likely to perform an operation in the presence of these Parts of Speech.} \\
\cline{1-1} 
\small WRB: WH-Adverb & \\
\cline{1-1} 
\small EX: Expletive & \\ 
\cline{1-1} 
\small RBR: Comparative Adverb & \\ 
\cline{1-1} 
\small RBS: Superlative Adverb & \\ 
\cline{1-1} 
\small VBD: Past tense Verb & \\ 
\cline{1-1} 
\small VB: Base form Verb & \\ 
\hline
\end{tabular}
\caption{Parts of Speech based Features}
\label{figure:10}
\end{table}
\vspace{0.15cm}
\subsubsection{Verb Similarity based}
A \textit{Positive Verb} is a verb which indicates that the subject in the sentence is gaining some quantified object. A \textit{Negative Verb} is a verb which indicates that the subject is losing something. We extract the most frequent verbs in  $\mathit{+}$ and $\mathit{-}$ labeled sentences. Based on the frequencies we extract 13 significantly differentiating verbs for each class. The similarity of the lemma of these verbs to the action verb in the sentence is then used as a feature. Therefore, we have a total of 26 such features. The similarity score is calculated based on the WUP word similarity using WordNet~\cite{WordNet:95}.

\section{Word Problem Solver}
\subsection{Using Operator Prediction Results}
Based on the predicted operators for each simplified sentence, we create a representation for every subject having one or more objects. Refer to Figure \ref{figure:11} for details:

\begin{table}[h!]
\centering
\begin{tabular}{ | m{7cm} | }
\hline
 $\mathbf{\small S}$\textbf{\small : Joan found 70 seashells on the beach . she gave Sam some of her seashells . She has 27 seashell . How many seashells did she give to Sam ?}\\
\hline
$\mathit{\small k_{1}}$\small : Joan found 70 seashells on the beach.\\
\hline
$\mathit{\small k_{2}}$\small : she gave Sam some of her seashells .\\
\hline
$\mathit{\small k_{3}}$\small : She has 27 seashell .\\
\hline
$\mathit{\small k_{4}}$\small : How many seashells did she give to Sam?\\
\hline
\end{tabular}
\caption{Example Word Problem}
\label{figure:11}
\end{table}

\vspace{0.25cm}
The representation of the above simplified sentences would be as shown in Figure \ref{figure:12}:
\begin{table}[h!]
\centering
\begin{tabular}{ |p{1.3cm}|p{1.7cm}|p{3.7cm}| }
\hline
\textbf{\small Sentence} & \textbf{\small Predicted Operator} & \textbf{\small Representation} \\ \hline
$\mathit{\small k_{1}}$ & $\mathit{\small +}$ & $\mathit{\small Joan \leftarrow 70}$\textit{\small seashell} \\ \hline
$\mathit{\small k_{2}}$ & $\mathit{\small -}$ & $\mathit{\small Joan \leftarrow 70 - X}$\textit{\small seashell} $\mathit{\small Sam \leftarrow +X}\textit{\small seashell}$ \\ \hline
$\mathit{\small k_{3}}$ & $\mathit{\small =}$ & $\mathit{\small Joan \leftarrow 70 - X = 27}$ \textit{\small seashell}\\ \hline
\end{tabular}
\caption{Word Problem State Representation}
\label{figure:12}
\end{table}
\vspace{0.25cm}
We use Spacy\footnote{https://spacy.io} to extract dependency relations and attempt to extract equation for a word problem based on the subject and object identified in the question sentence. There are 4 scenarios we consider:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item If the question sentence has a singular subject and an object, we map the subject to one of the entities in our representation and output the result.
\item If the question sentence has a plural subject (For Example: \textit{they}) and an object, we perform all the identified operations for that object.
\item If the question sentence has a comparative adjective and multiple subjects or multiple objects, we output the result by subtracting the smaller quantity from the greater one.
\item If the question sentence does not fall in any one of the above cases, we perform all the identified operations in our representation and output the result.
\end{enumerate}


\section{Experimental Results}
\subsection{Operator Prediction Classifier}
Out of 1218 simplified word problems, we use 1015 to train our classifier and the remaining 203 to test. 

\begin{table}[h!]
\centering
\begin{tabular}{|m{0.6cm}|m{1.2cm}|m{1.2cm}|m{1.2cm}|m{0.8cm}|}
\hline
\textbf{\small Class} & \textbf{\small Training Count} & \textbf{\small Testing Count} & \textbf{\small Precision} & \textbf{\small Recall} \\ \hline
$\mathit{\small +}$ & \small 1811 & \small 375 & \small 96.23 & \small 74.93 \\ \hline
$\mathit{\small -}$ & \small 528 & \small 102 & \small 85.57 & \small 87.25 \\ \hline
$\mathit{\small ?}$ & \small 1015 & \small 203 & \small 100 & \small 100 \\ \hline
$\mathit{\small =}$ & \small 113 & \small 28 & \small 25 & \small 53.57 \\ \hline
$\mathit{\small i}$ & \small 317 & \small 44 & \small 35.48 & \small 75 \\ \hline
\multicolumn{5}{|m{7cm}|}{\textbf{\small Accuracy: 82.57}\%} \\ \hline
\end{tabular}
\caption{Operator Prediction Results}
\label{figure:13}
\end{table}
\vspace{0.15cm}
We achieve nearly 75\% or more precision and recall for all three important operations.

\subsection{Word Problem Solver}

\begin{table}[h!]
\centering
\begin{tabular}{|m{2.9cm}|m{0.7cm}|m{0.7cm}|m{0.7cm}|m{0.7cm}|}
\hline
 & \textbf{\small MA1} & \textbf{\small IXL} & \textbf{\small MA2} & \textbf{\small Total} \\ \hline
\small ~\newcite{ARIS:14} & \small 83.6 & \small 75.0 & \small 74.4 & \small 77.7 \\ \hline
\small ~\newcite{RoyR:15} & - & - & - & \small 78.0 \\ \hline
\small ~\newcite{Kushman:14} & \small 89.6 & \small 51.1 & \small 51.2 & \small 64.0 \\ \hline
\textbf{\small Proposed Method} & \textbf{\small 97.8} & \small 59.28 & \textbf{\small 76.03} & 77.7 \\ \hline
\end{tabular}
\caption{Solver Results for AI2 Dataset}
\label{figure:14}
\end{table}
\vspace{0.3cm}
Overall, we perform as good as ~\cite{ARIS:14} and we achieve better results in 2 datasets because of our improvement in operator prediction. The word problems in \textbf{MA2} are comparatively complex, and hence our sentence simplification system needs improvement to simplify sentences more accurately.
\begin{table}[H]
\centering
\begin{tabular}{|m{2cm}|m{1.2cm}|m{1.2cm}|}
\hline
 \small MAWPS Dataset & \textbf{\small Training} & \textbf{\small Testing} \\ \hline
 \textbf{\small Count} & \small 1015 & \small 203 \\ \hline
 \textbf{\small Accuracy} & \small 90.14 & \textbf{\small 91.62} \\ \hline
\end{tabular}
\caption{Solver Results for MAWPS Dataset}
\label{figure:15}
\end{table}
\vspace{0.25cm}
We achieve exceptional results on the universal dataset for arithmetic word problems.
\vspace{0.1cm}
\section{Error Analysis}
\subsection{Sentence Simplification}
We analyzed the errors in sentence simplification system where minimal manual intervention was required to simplify the sentences correctly. We present our analysis in Figure \ref{figure:16}. 

\begin{table}[h!]
\centering
\begin{tabular}{|m{2cm}|m{2cm}|m{2cm}|}
\hline
\textbf{\small Error Type} & \textbf{\small Description} & \textbf{\small Example}  \\ \hline
\small Dataset Errors 2\% &\small Improper formed sentences in the dataset. & \small Joan has 5 apples Mary has 2 apples. \\ \hline
\small Compound Nouns 2\% & \small Unable to identify compound nouns. & \small Joan has 5 blue and 2 red marbles. \\ \hline
\small Conditional Beginnings 5\% & \small Sentences beginning with conditional words such as \textit{if} or \textit{when}. & \small If her fund was worth 1472 before , how much is it worth now ? \\ \hline
\small Parsing Errors 8\% & \small Uncertainty of dependency parser while parsing complex sentences. & \small Each year, salmon travels upstream, going from the ocean to the rivers where they were born.\\ \hline
\end{tabular}
\caption{Sentence Simplification Errors}
\label{figure:16}
\end{table}

\subsection{Solver}
There are 4 major classes of errors for the solver as shown in \ref{figure:17}. In the first category, the parser is not able to correctly identify the subject and object in the sentence. The second category refers to errors that require set completion knowledge. For example, played can be divided into win and lose. Also, we find irrelevant information with cardinals that add to errors in the solver. For example, to identify the count of cards, it is not required to know how many of them were torn.
\begin{table}[h!]
\centering
\begin{tabular}{|m{2cm}|m{4 cm}|}
\hline
 \textbf{\small Error Type} & \textbf{\small Example}\\ \hline
\small Parsing Issues 15\% & \small Sally had 27 Pokemon cards. Dan gave her 41 new Pokemon cards. How many Pokemon cards does Sally have now? \\ \hline
\small Set Completion 5\% &\small  Sara's school played 12 games this year. They won 4 games. How many games did they lose?. \\ \hline
\small Irrelevant Information 10\% & \small Sara has 20 baseball cards but 9 were torn. She gave 10 baseball cards to Joan. How many cards does she now have? \\ \hline
\small Others 10\% & \small In March it rained 0.81 inches. It rained 0.35 inches less in April than in March. How much did it rain in April?  \\ \hline
\end{tabular}
\caption{Solver Errors}
\label{figure:17}
\end{table}


\section{Conclusion}\label{sec:conclusion}
This paper presents a method for understanding and solving addition and subtraction arithmetic word problems. We develop a novel theoretical framework, centered around the notion of sentence simplification for operator predictions. We show this by developing a classifier that yields strong performance on several benchmark collections. Our approach also performs equally well on multistep problems, even when it has never observed a particular problem type before.

\bibliography{EACL_Paper.bib}
\bibliographystyle{eacl2017.bst}
\end{document}